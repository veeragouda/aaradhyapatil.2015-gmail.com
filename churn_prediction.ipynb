{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOoPG0V/rzhDkYUGn5va89E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veeragouda/aaradhyapatil.2015-gmail.com/blob/master/churn_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUXKPOci_vt9"
      },
      "source": [
        "### Python | Customer Churn Analysis Prediction\r\n",
        "Last Updated : 23 Mar, 2020\r\n",
        "Customer Churn\r\n",
        "It is when an existing customer, user, subscriber, or any kind of return client stops doing business or ends the relationship with a company.\r\n",
        "\r\n",
        "Types of Customer Churn –\r\n",
        "\r\n",
        "Contractual Churn : When a customer is under a contract for a service and decides to cancel the service e.g. Cable TV, SaaS.\r\n",
        "Voluntary Churn : When a user voluntarily cancels a service e.g. Cellular connection.\r\n",
        "Non-Contractual Churn : When a customer is not under a contract for a service and decides to cancel the service e.g. Consumer Loyalty in retail stores.\r\n",
        "Involuntary Churn : When a churn occurs without any request of the customer e.g. Credit card expiration.\r\n",
        "Reasons for Voluntary Churn\r\n",
        "\r\n",
        "Reasons for Voluntary Churn\r\n",
        "Lack of usage;\r\n",
        "Poor service;\r\n",
        "Better price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLQQOd2O_z9c"
      },
      "source": [
        "# Import required libraries \r\n",
        "import numpy as np \r\n",
        "import pandas as pd \r\n",
        "  \r\n",
        "# Import the dataset \r\n",
        "dataset = pd.read_csv('telcochurndata.csv') \r\n",
        "  \r\n",
        "# Glance at the first five records \r\n",
        "dataset.head() \r\n",
        "  \r\n",
        "# Print all the features of the data \r\n",
        "dataset.columns\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntHHrYrsAXV7"
      },
      "source": [
        "# Churners vs Non-Churners \r\n",
        "dataset['Churn'].value_counts() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHO5S9YFAgoY"
      },
      "source": [
        "Exploratory Data Analysis on Telco Churn Dataset\r\n",
        "\r\n",
        "Code : To find the number of churners and non-churners in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THJbFY4RAlV2"
      },
      "source": [
        "## Group data by 'Churn' and compute the mean \r\n",
        "print(dataset.groupby('Churn')['Customer service calls'].mean()) \r\n",
        "\r\n",
        "## To group data by Churn and compute the mean to find out if churners make more customer service calls than non-churners:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyO6zlcrAnP8"
      },
      "source": [
        "# Count the number of churners and non-churners by State \r\n",
        "print(dataset.groupby('State')['Churn'].value_counts()) \r\n",
        "\r\n",
        "## To find out if one State has more churners compared to another."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEYr8mZUApkm"
      },
      "source": [
        " ### Exploring Data Visualizations : To understand how variables are distributed.\r\n",
        "\r\n",
        "# Import matplotlib and seaborn \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "import seaborn as sns \r\n",
        "  \r\n",
        "# Visualize the distribution of 'Total day minutes' \r\n",
        "plt.hist(dataset['Total day minutes'], bins = 100) \r\n",
        "  \r\n",
        "# Display the plot \r\n",
        "plt.show() \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03yjTm1gApwu"
      },
      "source": [
        "# Create the box plot \r\n",
        "sns.boxplot(x = 'Churn', \r\n",
        "            y = 'Customer service calls', \r\n",
        "            data = dataset, \r\n",
        "            sym = \"\",                   \r\n",
        "            hue = \"International plan\")  \r\n",
        "# Display the plot \r\n",
        "plt.show()\r\n",
        "\r\n",
        "## o visualize the difference in Customer service calls between churners and non-churners"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Ep5OymAp7O"
      },
      "source": [
        "### Data Preprocessing for Telco Churn Dataset\r\n",
        "\r\n",
        "### Many Machine Learning models make certain assumptions about how the data is distributed. Some of the assumptions are as follows:\r\n",
        "\r\n",
        "The features are normally distributed\r\n",
        "The features are on the same scale\r\n",
        "The datatypes of features are numeric\r\n",
        "In telco churn data, Churn, Voice mail plan, and, International plan, in particular, are binary features that can easily be converted into 0’s and 1’s\r\n",
        "\r\n",
        "# Features and Labels \r\n",
        "X = dataset.iloc[:, 0:19].values \r\n",
        "y = dataset.iloc[:, 19].values # Churn \r\n",
        "  \r\n",
        "# Encoding categorical data in X \r\n",
        "from sklearn.preprocessing import LabelEncoder \r\n",
        "  \r\n",
        "labelencoder_X_1 = LabelEncoder() \r\n",
        "X[:, 3] = labelencoder_X_1.fit_transform(X[:, 3]) \r\n",
        "  \r\n",
        "labelencoder_X_2 = LabelEncoder() \r\n",
        "X[:, 4] = labelencoder_X_2.fit_transform(X[:, 4]) \r\n",
        "  \r\n",
        "# Encoding categorical data in y \r\n",
        "labelencoder_y = LabelEncoder() \r\n",
        "y = labelencoder_y.fit_transform(y) \r\n",
        "\r\n",
        "### Encoding State feature using One hot encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOyPXjmWAqFk"
      },
      "source": [
        "# Removing extra column to avoid dummy variable trap \r\n",
        "X_State = pd.get_dummies(X[:, 0], drop_first = True) \r\n",
        "  \r\n",
        "# Converting X to a dataframe \r\n",
        "X = pd.DataFrame(X) \r\n",
        "  \r\n",
        "# Dropping the 'State' column \r\n",
        "X = X.drop([0], axis = 1) \r\n",
        "  \r\n",
        "# Merging two dataframes \r\n",
        "frames = [X_State, X] \r\n",
        "result = pd.concat(frames, axis = 1, ignore_index = True) \r\n",
        "  \r\n",
        "# Final dataset with all numeric features \r\n",
        "X = result \r\n",
        "\r\n",
        "### To Create Training and Test sets\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOQ448-1AqPz"
      },
      "source": [
        "# Splitting the dataset into the Training and Test sets \r\n",
        "from sklearn.model_selection import train_test_split \r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,  \r\n",
        "                                                    test_size = 0.2,  \r\n",
        "                                                    random_state = 0) \r\n",
        "### To scale features of the training and test sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UBfnwDYAqXs"
      },
      "source": [
        "# Feature Scaling \r\n",
        "from sklearn.preprocessing import StandardScaler \r\n",
        "sc = StandardScaler() \r\n",
        "X_train = sc.fit_transform(X_train) \r\n",
        "X_test = sc.transform(X_test) \r\n",
        "### Code: To train a Random Forest classifier model on the training set."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuRuZha1Aqaf"
      },
      "source": [
        "# Import RandomForestClassifier \r\n",
        "from sklearn.ensemble import RandomForestClassifier \r\n",
        "  \r\n",
        "# Instantiate the classifier \r\n",
        "clf = RandomForestClassifier() \r\n",
        "  \r\n",
        "# Fit to the training data \r\n",
        "clf.fit(X_train, y_train) \r\n",
        "## Code : Making Predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2WiRPnuAqdX"
      },
      "source": [
        "# Predict the labels for the test set \r\n",
        "y_pred = clf.predict(X_test) \r\n",
        "Code: Evaluating Model Performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6JJBXiWAqgL"
      },
      "source": [
        "# Compute accuracy \r\n",
        "from sklearn.metrics import accuracy_score \r\n",
        "  \r\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0eD2WNOAqiy"
      },
      "source": [
        "## Confusion matrix\r\n",
        "from sklearn.metrics import confusion_matrix \r\n",
        "print(confusion_matrix(y_test, y_pred)) \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PllrsTkAqlW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}